{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pretty_midi\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load data list\n",
    "data_list = {}\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "with open(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\dataset\\400-100-dataset\\list.json', 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "train_list = data_list['train']\n",
    "test_list = data_list['test']\n",
    "\n",
    "with open(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\dataset\\v-a values\\arousal.json','r') as load_f:\n",
    "    arousal_dict = json.load(load_f)\n",
    "    \n",
    "with open(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\dataset\\v-a values\\volence.json','r') as load_f:\n",
    "    volence_dict = json.load(load_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = keras.models.load_model(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\best_crnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# dir\n",
    "config = r\"cat-mel_2bar_small\"\n",
    "train_dir = r'C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\400-100\\train'\n",
    "output_dir = r'C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output'\n",
    "checkpoint_file = r'C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\400-100\\cat-mel_2bar_small\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# run code in CMD and will print the output of IN TIME ref.: https://lumeng.blog.csdn.net/article/details/104845611\n",
    "def runCMD(cmd):\n",
    "    screenData = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "    while True:\n",
    "        line = screenData.stdout.readline()\n",
    "        print(line.decode('gbk').strip(\"b'\"))\n",
    "        if line == b'' or subprocess.Popen.poll(screenData) == 0:\n",
    "            screenData.stdout.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# split midi in to time fragment\n",
    "def split_midi_by_time(midi_data,frag_start,frag_finish):\n",
    "    new_midi = midi_data\n",
    "\n",
    "    for i in range(len(new_midi.instruments)):\n",
    "        inst = new_midi.instruments[i]\n",
    "        for n in range(len(inst.notes)-1,-1,-1):\n",
    "            note = inst.notes[n]\n",
    "            if ((note.end<frag_start) or (note.start>frag_finish)):\n",
    "                inst.notes.remove(inst.notes[n])\n",
    "        for n in range(len(inst.notes)-1,-1,-1):\n",
    "            note = inst.notes[n]\n",
    "            note.start = max(note.start - frag_start,0)\n",
    "            note.end = min(note.end - frag_start, frag_finish - frag_start)\n",
    "\n",
    "    return new_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# interpolate music\n",
    "def interpolate_music(train_dir,output_dir,music1,music2,frag1,frag2):\n",
    "    midi_data = midi_data = pretty_midi.PrettyMIDI(train_dir + '\\\\' + music1 + '.mp3.mid')\n",
    "    new_midi = split_midi_by_time(midi_data,(frag1/2+15),(frag1/2+15)+4)\n",
    "    interMusic_1 = output_dir + '\\\\'+ music1 + '_trim.mid'\n",
    "    new_midi.write(interMusic_1)\n",
    "    \n",
    "    midi_data = midi_data = pretty_midi.PrettyMIDI(train_dir + '\\\\' + music2 + '.mp3.mid')\n",
    "    new_midi = split_midi_by_time(midi_data,(frag2/2+15),(frag2/2+15)+4)\n",
    "    interMusic_2 = output_dir + '\\\\'+ music2 + '_trim.mid'\n",
    "    new_midi.write(interMusic_2)\n",
    "    \n",
    "    music_interpolate = r'music_vae_generate \\\n",
    "    --config={config} \\\n",
    "    --checkpoint_file={checkpoint_file} \\\n",
    "    --mode=interpolate \\\n",
    "    --num_outputs={num_outputs} \\\n",
    "    --input_midi_1={interMusic_1} \\\n",
    "    --input_midi_2={interMusic_2} \\\n",
    "    --output_dir={output_dir}'\n",
    "\n",
    "    runCMD(music_interpolate.\n",
    "              format(config=config,checkpoint_file=checkpoint_file,num_outputs=2,\n",
    "                    interMusic_1=interMusic_1, interMusic_2=interMusic_2,output_dir=output_dir))\n",
    "\n",
    "    output_num = 0\n",
    "    for parent,dirnames, filenames in os.walk(output_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.find(\"cat-mel_2bar_small\")!=-1:\n",
    "                newname = \"{music1}-{frag1}th-{frag14}th_{music2}-{frag2}th-{frag24}th_{output_num}-of-2\".format(\n",
    "                    music1=music1,frag1=frag1,frag14=frag1+8,music2=music2,frag2=frag2,frag24=frag2+8,\n",
    "                        output_num=output_num)\n",
    "                new_midi_name = os.path.join(parent,newname + \".mid\")\n",
    "                new_wav_name = os.path.join(parent,newname + \".wav\")\n",
    "                \n",
    "                os.rename(os.path.join(parent,filename),new_midi_name)\n",
    "                output_num+=1\n",
    "                \n",
    "                runCMD(r'TiMidity\\timidity.exe {midifile} -Ow -o {wavfile}'.format(\n",
    "                    midifile = new_midi_name, wavfile = new_wav_name))\n",
    "        \n",
    "        \n",
    "    os.remove(interMusic_1)\n",
    "    os.remove(interMusic_2)\n",
    "    \n",
    "    return newname[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# run the model on the output.wav\n",
    "def evaluate_wav_output(model,output_dir,inter_result):\n",
    "    result = []\n",
    "    for parent, dirnames, filenames in os.walk(output_dir,  followlinks=True):\n",
    "        for filename in filenames:\n",
    "            if (filename.endswith(\".wav\") and (filename.find(inter_result)!=-1)):\n",
    "                index = os.path.splitext(filename)[0]\n",
    "#                 print(os.path.join(output_dir, filename))\n",
    "                wavsong = AudioSegment.from_wav(os.path.join(output_dir, filename))\n",
    "\n",
    "                one_data = []\n",
    "                test_data = []\n",
    "\n",
    "                # mel freq\n",
    "                for i in range(4):\n",
    "                    outputpath = os.path.join(output_dir,index+'-'+str(i)+\".wav\")\n",
    "                    wavsong[(500*i):(500+500*i)].export(outputpath, format=\"wav\")\n",
    "                    y, sr = librosa.load(outputpath, sr=None)\n",
    "                    os.remove(outputpath)\n",
    "\n",
    "                    fragment = np.array(\n",
    "                                    np.log(\n",
    "                                        librosa.feature.melspectrogram(\n",
    "                                            y, sr, n_mels=128,n_fft=int(sr*0.5), hop_length=int(sr*1)\n",
    "                                        )\n",
    "                                    ).transpose())\n",
    "                    fragment = fragment[np.newaxis,:]\n",
    "\n",
    "                    if one_data==[]:\n",
    "                        one_data = fragment.copy()                    \n",
    "                    else:\n",
    "                        one_data = np.vstack([one_data,fragment.copy()])\n",
    "\n",
    "                # self repliction\n",
    "                for i in range(15):\n",
    "                    if test_data==[]:\n",
    "                        test_data = one_data.copy()                    \n",
    "                    else:\n",
    "                        test_data = np.vstack([one_data,test_data.copy()])                    \n",
    "\n",
    "                # crnn test\n",
    "                test_data = test_data.reshape(1,60,128,1)\n",
    "                predict_va = model.predict(test_data)\n",
    "                result.append([sum(sum(predict_va[0])/60),sum(sum(predict_va[1])/60)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# write result into csv file\n",
    "def write_result(record_csv,inter_result,eva_result):\n",
    "    aver_eva_result = [0,0]\n",
    "    for i in range(len(eva_result)):\n",
    "        for j in range(2):\n",
    "            aver_eva_result[j] += eva_result[i][j]\n",
    "    aver_eva_result = [i/len(eva_result) for i in aver_eva_result]\n",
    "    print(aver_eva_result)\n",
    "    \n",
    "    inter_result_format = \"{music1}-{frag1}th-{frag14}th_{music2}-{frag2}th-{frag24}th\"\n",
    "    parsed = parse.parse(inter_result_format,inter_result)\n",
    "    \n",
    "    # music1\n",
    "    aver_va_music1 = [0,0]\n",
    "    frag1 = int(parsed['frag1'])\n",
    "    music1 = int(parsed['music1'])\n",
    "    for i in range(frag1,frag1+8):\n",
    "        aver_va_music1[0]+=volence_dict[str(music1)][str(i)]\n",
    "        aver_va_music1[1]+=arousal_dict[str(music1)][str(i)]\n",
    "    aver_va_music1[0]/=8\n",
    "    aver_va_music1[1]/=8\n",
    "    \n",
    "    aver_va_music2 = [0,0]\n",
    "    frag2 = int(parsed['frag2'])\n",
    "    music2 = int(parsed['music2'])\n",
    "    for i in range(frag2,frag2+8):\n",
    "        aver_va_music2[0]+=volence_dict[str(music2)][str(i)]\n",
    "        aver_va_music2[1]+=arousal_dict[str(music2)][str(i)]\n",
    "    aver_va_music2[0]/=8\n",
    "    aver_va_music2[1]/=8\n",
    "    \n",
    "    print(aver_va_music1,aver_va_music2)\n",
    "    \n",
    "    with open(record_csv,\"a+\") as f:\n",
    "        f.write(\"{0},{1},{2},{3},{4},{5},{6}\\n\".format(\n",
    "                inter_result,\n",
    "                aver_va_music1[0],aver_va_music1[1],aver_va_music2[0],aver_va_music2[1],\n",
    "                aver_eva_result[0],aver_eva_result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1104 10:00:27.805285 36268 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1979_trim.mid`. Outputting these potential inputs as `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\cat-mel_2bar_small_input1-extractions_2020-11-04_100027-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1979-15th-23th_1663-10th-18th_0-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1979-15th-23th_1663-10th-18th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1979-15th-23th_1663-10th-18th_1-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1979-15th-23th_1663-10th-18th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-15th-23th_1663-10th-18th [[-0.21642934532428626, -0.13960635674447985], [-0.17366275082167704, -0.16782739049449447]]\n",
      "[-0.19504604807298165, -0.15371687361948716]\n",
      "[0.16275, 0.5746249999999999] [0.018500000000000003, 0.415]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1104 10:00:37.750100 17464 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1556_trim.mid`. Outputting these potential inputs as `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\cat-mel_2bar_small_input1-extractions_2020-11-04_100037-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1556-4th-12th_1874-7th-15th_0-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1556-4th-12th_1874-7th-15th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1556-4th-12th_1874-7th-15th_1-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1556-4th-12th_1874-7th-15th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "1556-4th-12th_1874-7th-15th [[-0.1301451944436849, -0.18848085435456596], [-0.18403924271115102, -0.10860040600528009]]\n",
      "[-0.15709221857741795, -0.14854063017992303]\n",
      "[0.09162499999999998, 0.47725] [0.257375, 0.37912499999999993]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-00fa28f557d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfrag_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m52\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfrag_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m52\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minter_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterpolate_music\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmusic_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmusic_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrag_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrag_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0meva_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_wav_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minter_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minter_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meva_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-a7b23099cdca>\u001b[0m in \u001b[0;36minterpolate_music\u001b[1;34m(train_dir, output_dir, music1, music2, frag1, frag2)\u001b[0m\n\u001b[0;32m     15\u001b[0m     runCMD(music_interpolate.\n\u001b[0;32m     16\u001b[0m               format(config=config,checkpoint_file=checkpoint_file,num_outputs=2,\n\u001b[1;32m---> 17\u001b[1;33m                     interMusic_1=interMusic_1, interMusic_2=interMusic_2,output_dir=output_dir))\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0moutput_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0bcc48e00e62>\u001b[0m in \u001b[0;36mrunCMD\u001b[1;34m(cmd)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mscreenData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscreenData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gbk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb''\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreenData\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testtimes = 5000\n",
    "for i in range(testtimes):\n",
    "    [music_1,music_2] = random.sample(train_list,2)\n",
    "    frag_1 = random.randint(0,52)\n",
    "    frag_2 = random.randint(0,52)\n",
    "    inter_result = interpolate_music(train_dir,output_dir,music_1,music_2,frag_1,frag_2)\n",
    "    eva_result = evaluate_wav_output(model,output_dir,inter_result)\n",
    "    print(inter_result,eva_result)\n",
    "    write_result(r\"C:\\Users\\Arnold\\Desktop\\4560\\Project\\data\\3-3. Emotion Changing result\\label_changing.csv\",inter_result,eva_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

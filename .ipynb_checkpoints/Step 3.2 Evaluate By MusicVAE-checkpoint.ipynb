{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pretty_midi\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load data list\n",
    "data_list = {}\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "with open(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\dataset\\400-100-dataset\\list.json', 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "train_list = data_list['train']\n",
    "test_list = data_list['test']\n",
    "\n",
    "with open(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\dataset\\v-a values\\arousal.json','r') as load_f:\n",
    "    arousal_dict = json.load(load_f)\n",
    "    \n",
    "with open(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\dataset\\v-a values\\volence.json','r') as load_f:\n",
    "    volence_dict = json.load(load_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = keras.models.load_model(r'C:\\Users\\Arnold\\Desktop\\4560\\CNN\\best_crnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dir\n",
    "train_dir = r'C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\400-100\\train'\n",
    "output_dir = r'C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output'\n",
    "checkpoint_file = r'C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\400-100\\cat-mel_2bar_small\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# run code in CMD and will print the output of IN TIME ref.: https://lumeng.blog.csdn.net/article/details/104845611\n",
    "def runCMD(cmd):\n",
    "    screenData = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "    while True:\n",
    "        line = screenData.stdout.readline()\n",
    "        print(line.decode('gbk').strip(\"b'\"))\n",
    "        if line == b'' or subprocess.Popen.poll(screenData) == 0:\n",
    "            screenData.stdout.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# split midi in to time fragment\n",
    "def split_midi_by_time(midi_data,frag_start,frag_finish):\n",
    "    new_midi = midi_data\n",
    "\n",
    "    for i in range(len(new_midi.instruments)):\n",
    "        inst = new_midi.instruments[i]\n",
    "        for n in range(len(inst.notes)-1,-1,-1):\n",
    "            note = inst.notes[n]\n",
    "            if ((note.end<frag_start) or (note.start>frag_finish)):\n",
    "                inst.notes.remove(inst.notes[n])\n",
    "        for n in range(len(inst.notes)-1,-1,-1):\n",
    "            note = inst.notes[n]\n",
    "            note.start = max(note.start - frag_start,0)\n",
    "            note.end = min(note.end - frag_start, frag_finish - frag_start)\n",
    "\n",
    "    return new_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# interpolate music\n",
    "def interpolate_music(train_dir,output_dir,music1,music2,frag1,frag2):\n",
    "    midi_data = midi_data = pretty_midi.PrettyMIDI(train_dir + '\\\\' + music1 + '.mp3.mid')\n",
    "    new_midi = split_midi_by_time(midi_data,(frag1/2+15),(frag1/2+15)+4)\n",
    "    interMusic_1 = output_dir + '\\\\'+ music1 + '_trim.mid'\n",
    "    new_midi.write(interMusic_1)\n",
    "    \n",
    "    midi_data = midi_data = pretty_midi.PrettyMIDI(train_dir + '\\\\' + music2 + '.mp3.mid')\n",
    "    new_midi = split_midi_by_time(midi_data,(frag2/2+15),(frag2/2+15)+4)\n",
    "    interMusic_2 = output_dir + '\\\\'+ music2 + '_trim.mid'\n",
    "    new_midi.write(interMusic_2)\n",
    "    \n",
    "    music_interpolate = r'music_vae_generate \\\n",
    "    --config={config} \\\n",
    "    --checkpoint_file={checkpoint_file} \\\n",
    "    --mode=interpolate \\\n",
    "    --num_outputs={num_outputs} \\\n",
    "    --input_midi_1={interMusic_1} \\\n",
    "    --input_midi_2={interMusic_2} \\\n",
    "    --output_dir={output_dir}'\n",
    "\n",
    "    runCMD(music_interpolate.\n",
    "              format(config=config,checkpoint_file=checkpoint_file,num_outputs=8,\n",
    "                    interMusic_1=interMusic_1, interMusic_2=interMusic_2,output_dir=output_dir))\n",
    "\n",
    "    output_num = 0\n",
    "    for parent,dirnames, filenames in os.walk(output_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.find(\"cat-mel_2bar_small\")!=-1:\n",
    "                newname = \"{music1}-{frag1}th-{frag14}th_{music2}-{frag2}th-{frag24}th_{output_num}-of-2\".format(\n",
    "                    music1=music1,frag1=frag1,frag14=frag1+8,music2=music2,frag2=frag2,frag24=frag2+8,\n",
    "                        output_num=output_num)\n",
    "                new_midi_name = os.path.join(parent,newname + \".mid\")\n",
    "                new_wav_name = os.path.join(parent,newname + \".wav\")\n",
    "                \n",
    "                os.rename(os.path.join(parent,filename),new_midi_name)\n",
    "                output_num+=1\n",
    "                \n",
    "                runCMD(r'TiMidity\\timidity.exe {midifile} -Ow -o {wavfile}'.format(\n",
    "                    midifile = new_midi_name, wavfile = new_wav_name))\n",
    "        \n",
    "        \n",
    "    os.remove(interMusic_1)\n",
    "    os.remove(interMusic_2)\n",
    "    \n",
    "    return newname[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# run the model on the output.wav\n",
    "def evaluate_wav_output(model,output_dir,inter_result):\n",
    "    result = []\n",
    "    for parent, dirnames, filenames in os.walk(output_dir,  followlinks=True):\n",
    "        for filename in filenames:\n",
    "            if (filename.endswith(\".wav\") and (filename.find(inter_result)!=-1)):\n",
    "                index = os.path.splitext(filename)[0]\n",
    "#                 print(os.path.join(output_dir, filename))\n",
    "                wavsong = AudioSegment.from_wav(os.path.join(output_dir, filename))\n",
    "\n",
    "                one_data = []\n",
    "                test_data = []\n",
    "\n",
    "                # mel freq\n",
    "                for i in range(4):\n",
    "                    outputpath = os.path.join(output_dir,index+'-'+str(i)+\".wav\")\n",
    "                    wavsong[(500*i):(500+500*i)].export(outputpath, format=\"wav\")\n",
    "                    y, sr = librosa.load(outputpath, sr=None)\n",
    "                    os.remove(outputpath)\n",
    "\n",
    "                    fragment = np.array(\n",
    "                                    np.log(\n",
    "                                        librosa.feature.melspectrogram(\n",
    "                                            y, sr, n_mels=128,n_fft=int(sr*0.5), hop_length=int(sr*1)\n",
    "                                        )\n",
    "                                    ).transpose())\n",
    "                    fragment = fragment[np.newaxis,:]\n",
    "\n",
    "                    if one_data==[]:\n",
    "                        one_data = fragment.copy()                    \n",
    "                    else:\n",
    "                        one_data = np.vstack([one_data,fragment.copy()])\n",
    "\n",
    "                # self repliction\n",
    "                for i in range(15):\n",
    "                    if test_data==[]:\n",
    "                        test_data = one_data.copy()                    \n",
    "                    else:\n",
    "                        test_data = np.vstack([one_data,test_data.copy()])                    \n",
    "\n",
    "                # crnn test\n",
    "                test_data = test_data.reshape(1,60,128,1)\n",
    "                predict_va = model.predict(test_data)\n",
    "                result.append([sum(sum(predict_va[0])/60),sum(sum(predict_va[1])/60)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# write result into csv file\n",
    "def write_result(record_csv,inter_result,eva_result):\n",
    "    aver_eva_result = [0,0]\n",
    "    for i in range(len(eva_result)):\n",
    "        for j in range(2):\n",
    "            aver_eva_result[j] += eva_result[i][j]\n",
    "    aver_eva_result = [i/len(eva_result) for i in aver_eva_result]\n",
    "    print(aver_eva_result)\n",
    "    \n",
    "    inter_result_format = \"{music1}-{frag1}th-{frag14}th_{music2}-{frag2}th-{frag24}th\"\n",
    "    parsed = parse.parse(inter_result_format,inter_result)\n",
    "    \n",
    "    # music1\n",
    "    aver_va_music1 = [0,0]\n",
    "    frag1 = int(parsed['frag1'])\n",
    "    music1 = int(parsed['music1'])\n",
    "    for i in range(frag1,frag1+8):\n",
    "        aver_va_music1[0]+=volence_dict[str(music1)][str(i)]\n",
    "        aver_va_music1[1]+=arousal_dict[str(music1)][str(i)]\n",
    "    aver_va_music1[0]/=8\n",
    "    aver_va_music1[1]/=8\n",
    "    \n",
    "    aver_va_music2 = [0,0]\n",
    "    frag2 = int(parsed['frag2'])\n",
    "    music2 = int(parsed['music2'])\n",
    "    for i in range(frag2,frag2+8):\n",
    "        aver_va_music2[0]+=volence_dict[str(music2)][str(i)]\n",
    "        aver_va_music2[1]+=arousal_dict[str(music2)][str(i)]\n",
    "    aver_va_music2[0]/=8\n",
    "    aver_va_music2[1]/=8\n",
    "    \n",
    "    print(aver_va_music1,aver_va_music2)\n",
    "    \n",
    "    with open(record_csv,\"a+\") as f:\n",
    "        f.write(\"{0},{1},{2},{3},{4},{5},{6}\\n\".format(\n",
    "                inter_result,\n",
    "                aver_va_music1[0],aver_va_music1[1],aver_va_music2[0],aver_va_music2[1],\n",
    "                aver_eva_result[0],aver_eva_result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1103 23:26:31.249005 39280 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\795_trim.mid`. Outputting these potential inputs as `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\cat-mel_2bar_small_input1-extractions_2020-11-03_232631-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\795-44th-52th_988-52th-60th_0-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\795-44th-52th_988-52th-60th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~9 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\795-44th-52th_988-52th-60th_1-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\795-44th-52th_988-52th-60th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795-44th-52th_988-52th-60th [[-0.20909428625600412, -0.09870241217504372], [-0.18342653461149894, -0.08962491754209623]]\n",
      "[-0.19626041043375153, -0.09416366485856997]\n",
      "[-0.224502225, -0.37121675000000004] [0.18541869625, 0.19765313750000002]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1103 23:26:42.284704 20596 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1088_trim.mid`. Outputting these potential inputs as `C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\cat-mel_2bar_small_input1-extractions_2020-11-03_232642-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1088-3th-11th_520-18th-26th_0-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1088-3th-11th_520-18th-26th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1088-3th-11th_520-18th-26th_1-of-2.mid\n",
      "\n",
      "MIDI file: C:\\Users\\Arnold\\Desktop\\4560\\MusicVAE\\DEAM\\practice\\output\\1088-3th-11th_520-18th-26th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "1088-3th-11th_520-18th-26th [[-0.21648526249919087, -0.09700071088809636], [-0.17252613443997689, -0.0917805048666196]]\n",
      "[-0.19450569846958388, -0.09439060787735798]\n",
      "[0.5453749999999999, 0.434] [-0.050865258749999996, -0.225405675]\n"
     ]
    }
   ],
   "source": [
    "testtimes = 1000\n",
    "for i in range(testtimes):\n",
    "    [music_1,music_2] = random.sample(train_list,2)\n",
    "    frag_1 = random.randint(0,52)\n",
    "    frag_2 = random.randint(0,52)\n",
    "    inter_result = interpolate_music(train_dir,output_dir,music_1,music_2,frag_1,frag_2)\n",
    "    eva_result = evaluate_wav_output(model,output_dir,inter_result)\n",
    "    print(inter_result,eva_result)\n",
    "    write_result(r\"C:\\Users\\Arnold\\Desktop\\4560\\Project\\data\\3-3. Emotion Changing result\\label_changing.csv\",inter_result,eva_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

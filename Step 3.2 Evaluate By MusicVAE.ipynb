{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pretty_midi\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load data list\n",
    "data_list = {}\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "with open(r'data\\1. 400-100 dataset\\list.json', 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "train_list = data_list['train']\n",
    "test_list = data_list['test']\n",
    "\n",
    "with open(r'data\\0. DEAM\\annotations\\arousal\\arousal.json','r') as load_f:\n",
    "    arousal_dict = json.load(load_f)\n",
    "    \n",
    "with open(r'data\\0. DEAM\\annotations\\valence\\valence.json','r') as load_f:\n",
    "    valence_dict = json.load(load_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = keras.models.load_model(r'data\\2. Best Model\\best_crnn_mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# dir\n",
    "config = r'cat-mel_2bar_small'\n",
    "train_dir = r'data\\1. 400-100 dataset\\train'\n",
    "output_dir = r'data\\3-2. Output from MusicVAE'\n",
    "checkpoint_file = r'data\\3-1. MusicVAE checkpoint\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# run code in CMD and will print the output of IN TIME ref.: https://lumeng.blog.csdn.net/article/details/104845611\n",
    "def runCMD(cmd):\n",
    "    \"\"\"\n",
    "    This function will run cmd in jupyter and print the output of cmd on time.\n",
    "    \n",
    "    Parameter Description:\n",
    "    cmd: cmd code.\n",
    "    \"\"\"\n",
    "    screenData = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "    while True:\n",
    "        line = screenData.stdout.readline()\n",
    "        print(line.decode('gbk').strip(\"b'\"))\n",
    "        if line == b'' or subprocess.Popen.poll(screenData) == 0:\n",
    "            screenData.stdout.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# split midi in to time fragment\n",
    "def split_midi_by_time(midi_data,frag_start,frag_finish):\n",
    "    \"\"\"\n",
    "    This function will get the time fragment of a midi data.\n",
    "    \n",
    "    Parameter Description:\n",
    "    midi_data: one midi data of Pretty_MIDI.\n",
    "    frag_start: the start second of the fragment(include).\n",
    "    frag_finish: the finish second of the fragment(not include).\n",
    "    \"\"\"\n",
    "    new_midi = midi_data\n",
    "\n",
    "    for i in range(len(new_midi.instruments)):\n",
    "        inst = new_midi.instruments[i]\n",
    "        for n in range(len(inst.notes)-1,-1,-1):\n",
    "            note = inst.notes[n]\n",
    "            if ((note.end<frag_start) or (note.start>frag_finish)):\n",
    "                inst.notes.remove(inst.notes[n])\n",
    "        for n in range(len(inst.notes)-1,-1,-1):\n",
    "            note = inst.notes[n]\n",
    "            note.start = max(note.start - frag_start,0)\n",
    "            note.end = min(note.end - frag_start, frag_finish - frag_start)\n",
    "\n",
    "    return new_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# interpolate music\n",
    "def interpolate_music(train_dir,output_dir,music1,music2,frag1,frag2):\n",
    "    \"\"\"\n",
    "    This function will using the 2 4-seconds fragments of 2 music and generate a new music.\n",
    "    \n",
    "    Parameter Description:\n",
    "    train_dir: the folder where the midi file is.\n",
    "    output_dir: the folder for temp midi file.\n",
    "    music1: the file name of first music.\n",
    "    music2: the file name of second music.\n",
    "    frag1: the start time of the fragment of music1.\n",
    "    frag2: the start tume of the fragment of music2.\n",
    "    \"\"\"\n",
    "    midi_data = midi_data = pretty_midi.PrettyMIDI(train_dir + '\\\\' + music1 + '.mp3.mid')\n",
    "    new_midi = split_midi_by_time(midi_data,(frag1/2+15),(frag1/2+15)+4)\n",
    "    interMusic_1 = output_dir + '\\\\'+ music1 + '_trim.mid'\n",
    "    new_midi.write(interMusic_1)\n",
    "    \n",
    "    midi_data = midi_data = pretty_midi.PrettyMIDI(train_dir + '\\\\' + music2 + '.mp3.mid')\n",
    "    new_midi = split_midi_by_time(midi_data,(frag2/2+15),(frag2/2+15)+4)\n",
    "    interMusic_2 = output_dir + '\\\\'+ music2 + '_trim.mid'\n",
    "    new_midi.write(interMusic_2)\n",
    "    \n",
    "    music_interpolate = r'music_vae_generate \\\n",
    "    --config={config} \\\n",
    "    --checkpoint_file=\"{checkpoint_file}\" \\\n",
    "    --mode=interpolate \\\n",
    "    --num_outputs={num_outputs} \\\n",
    "    --input_midi_1=\"{interMusic_1}\" \\\n",
    "    --input_midi_2=\"{interMusic_2}\" \\\n",
    "    --output_dir=\"{output_dir}\"'\n",
    "\n",
    "    runCMD(music_interpolate.\n",
    "              format(config=config,checkpoint_file=checkpoint_file,num_outputs=8,\n",
    "                    interMusic_1=interMusic_1, interMusic_2=interMusic_2,output_dir=output_dir))\n",
    "\n",
    "    output_num = 0\n",
    "    for parent,dirnames, filenames in os.walk(output_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.find(\"cat-mel_2bar_small\")!=-1:\n",
    "                newname = \"{music1}-{frag1}th-{frag14}th_{music2}-{frag2}th-{frag24}th_{output_num}-of-2\".format(\n",
    "                    music1=music1,frag1=frag1,frag14=frag1+8,music2=music2,frag2=frag2,frag24=frag2+8,\n",
    "                        output_num=output_num)\n",
    "                new_midi_name = os.path.join(parent,newname + \".mid\")\n",
    "                new_wav_name = os.path.join(parent,newname + \".wav\")\n",
    "                \n",
    "                os.rename(os.path.join(parent,filename),new_midi_name)\n",
    "                output_num+=1\n",
    "                \n",
    "                runCMD(r'\"external lib\\TiMidity\\timidity.exe\" \"{midifile}\" -Ow -o \"{wavfile}\"'.format(\n",
    "                    midifile = new_midi_name, wavfile = new_wav_name))\n",
    "        \n",
    "        \n",
    "    os.remove(interMusic_1)\n",
    "    os.remove(interMusic_2)\n",
    "    \n",
    "    return newname[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# run the model on the output.wav\n",
    "def evaluate_wav_output(model,output_dir,inter_result):\n",
    "    \"\"\"\n",
    "    This funcion will evaluate the interpolation result by one model.\n",
    "    \n",
    "    Parameter Descriptoin:\n",
    "    model: the folder where the model is.\n",
    "    output_dir: the folder where the result is.\n",
    "    inter_result: the name of the interpolation result.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for parent, dirnames, filenames in os.walk(output_dir,  followlinks=True):\n",
    "        for filename in filenames:\n",
    "            if (filename.endswith(\".wav\") and (filename.find(inter_result)!=-1)):\n",
    "                index = os.path.splitext(filename)[0]\n",
    "#                 print(os.path.join(output_dir, filename))\n",
    "                wavsong = AudioSegment.from_wav(os.path.join(output_dir, filename))\n",
    "\n",
    "                one_data = []\n",
    "                test_data = []\n",
    "\n",
    "                # mel freq\n",
    "                for i in range(4):\n",
    "                    outputpath = os.path.join(output_dir,index+'-'+str(i)+\".wav\")\n",
    "                    wavsong[(500*i):(500+500*i)].export(outputpath, format=\"wav\")\n",
    "                    y, sr = librosa.load(outputpath, sr=None)\n",
    "                    os.remove(outputpath)\n",
    "\n",
    "                    fragment = np.array(\n",
    "                                    np.log(\n",
    "                                        librosa.feature.melspectrogram(\n",
    "                                            y, sr, n_mels=128,n_fft=int(sr*0.5), hop_length=int(sr*1)\n",
    "                                        )\n",
    "                                    ).transpose())\n",
    "                    fragment = fragment[np.newaxis,:]\n",
    "\n",
    "                    if one_data==[]:\n",
    "                        one_data = fragment.copy()                    \n",
    "                    else:\n",
    "                        one_data = np.vstack([one_data,fragment.copy()])\n",
    "\n",
    "                # self repliction\n",
    "                for i in range(15):\n",
    "                    if test_data==[]:\n",
    "                        test_data = one_data.copy()                    \n",
    "                    else:\n",
    "                        test_data = np.vstack([one_data,test_data.copy()])                    \n",
    "\n",
    "                # crnn test\n",
    "                test_data = test_data.reshape(1,60,128,1)\n",
    "                predict_va = model.predict(test_data)\n",
    "                result.append([sum(sum(predict_va[0])/60),sum(sum(predict_va[1])/60)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# write result into csv file\n",
    "def write_result(record_csv,inter_result,eva_result):\n",
    "    \"\"\"\n",
    "    This function will record the result into a csv.\n",
    "    \n",
    "    Paramater Description:\n",
    "    record_csv: the full direction of the record csv.\n",
    "    inter_result: A string, the name of the interpolation result. To parse this string, we can learn what this result is interpolated from.\n",
    "    eva_result: A list of pair of real numbers, the evalation result of the interpolation result.\n",
    "    \"\"\"\n",
    "    aver_eva_result = [0,0]\n",
    "    for i in range(len(eva_result)):\n",
    "        for j in range(2):\n",
    "            aver_eva_result[j] += eva_result[i][j]\n",
    "    aver_eva_result = [i/len(eva_result) for i in aver_eva_result]\n",
    "    print(aver_eva_result)\n",
    "    \n",
    "    inter_result_format = \"{music1}-{frag1}th-{frag14}th_{music2}-{frag2}th-{frag24}th\"\n",
    "    parsed = parse.parse(inter_result_format,inter_result)\n",
    "    \n",
    "    # music1\n",
    "    aver_va_music1 = [0,0]\n",
    "    frag1 = int(parsed['frag1'])\n",
    "    music1 = int(parsed['music1'])\n",
    "    for i in range(frag1,frag1+8):\n",
    "        aver_va_music1[0]+=valence_dict[str(music1)][str(i)]\n",
    "        aver_va_music1[1]+=arousal_dict[str(music1)][str(i)]\n",
    "    aver_va_music1[0]/=8\n",
    "    aver_va_music1[1]/=8\n",
    "    \n",
    "    aver_va_music2 = [0,0]\n",
    "    frag2 = int(parsed['frag2'])\n",
    "    music2 = int(parsed['music2'])\n",
    "    for i in range(frag2,frag2+8):\n",
    "        aver_va_music2[0]+=valence_dict[str(music2)][str(i)]\n",
    "        aver_va_music2[1]+=arousal_dict[str(music2)][str(i)]\n",
    "    aver_va_music2[0]/=8\n",
    "    aver_va_music2[1]/=8\n",
    "    \n",
    "    print(aver_va_music1,aver_va_music2)\n",
    "    \n",
    "    with open(record_csv,\"a+\") as f:\n",
    "        f.write(\"{0},{1},{2},{3},{4},{5},{6}\\n\".format(\n",
    "                inter_result,\n",
    "                aver_va_music1[0],aver_va_music1[1],aver_va_music2[0],aver_va_music2[1],\n",
    "                aver_eva_result[1],aver_eva_result[0]))\n",
    "        # NOTE: The first is VOLENCE and the second is Arousal in the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"external lib\\TiMidity\\TIMIDITY.cfg\",\"w+\") as f:\n",
    "    f.write(\"soundfont \\\"{}\\\\external lib\\\\TiMidity\\\\GeneralUser.SF2\\\"\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1105 22:30:40.055764  3456 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `data\\3-2. Output from MusicVAE\\1130_trim.mid`. Outputting these potential inputs as `data\\3-2. Output from MusicVAE\\cat-mel_2bar_small_input1-extractions_2020-11-05_223040-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_0-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_1-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_2-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_2-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~9 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_3-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1130-34th-42th_1299-11th-19th_3-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~7 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130-34th-42th_1299-11th-19th [[0.1308589900543211, -0.0906238980451235], [0.017510488348307263, -0.24188366648741066], [0.20207755191950127, -0.32339234836399555], [0.28112606549984775, -0.3941610148176551]]\n",
      "[0.15789327395549435, -0.2625152319285462]\n",
      "[0.0955, -0.017124999999999998] [0.057999999999999996, 0.233125]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1105 22:30:51.669737 13036 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `data\\3-2. Output from MusicVAE\\1583_trim.mid`. Outputting these potential inputs as `data\\3-2. Output from MusicVAE\\cat-mel_2bar_small_input1-extractions_2020-11-05_223051-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1583-14th-22th_481-32th-40th_0-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1583-14th-22th_481-32th-40th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1583-14th-22th_481-32th-40th_1-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1583-14th-22th_481-32th-40th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "1583-14th-22th_481-32th-40th [[0.17954794138495345, -0.06146566554889432], [0.09824003301946505, -0.18431810743641108]]\n",
      "[0.13889398720220925, -0.1228918864926527]\n",
      "[0.276125, 0.11412499999999999] [0.0938950625, -0.2414781625]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1105 22:31:01.325929 12820 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `data\\3-2. Output from MusicVAE\\1436_trim.mid`. Outputting these potential inputs as `data\\3-2. Output from MusicVAE\\cat-mel_2bar_small_input1-extractions_2020-11-05_223101-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1436-8th-16th_795-37th-45th_0-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1436-8th-16th_795-37th-45th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1436-8th-16th_795-37th-45th_1-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1436-8th-16th_795-37th-45th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "1436-8th-16th_795-37th-45th [[0.14399256701290142, -0.11802036134213267], [0.12406145285058301, -0.1598767599207349]]\n",
      "[0.1340270099317422, -0.1389485606314338]\n",
      "[0.137375, 0.1555] [-0.21757092500000003, -0.32620425000000003]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1105 22:31:10.793429  6872 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `data\\3-2. Output from MusicVAE\\1739_trim.mid`. Outputting these potential inputs as `data\\3-2. Output from MusicVAE\\cat-mel_2bar_small_input1-extractions_2020-11-05_223110-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1739-21th-29th_515-46th-54th_0-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1739-21th-29th_515-46th-54th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~9 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\1739-21th-29th_515-46th-54th_1-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\1739-21th-29th_515-46th-54th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~7 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "1739-21th-29th_515-46th-54th [[0.13623011467279866, -0.08385343453846872], [0.10478696100472007, -0.12501494935713708]]\n",
      "[0.12050853783875937, -0.1044341919478029]\n",
      "[0.036375000000000005, 0.018000000000000002] [-0.282232625, -0.14081270625]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1105 22:31:20.404580  8952 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `data\\3-2. Output from MusicVAE\\667_trim.mid`. Outputting these potential inputs as `data\\3-2. Output from MusicVAE\\cat-mel_2bar_small_input1-extractions_2020-11-05_223120-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\667-13th-21th_1220-24th-32th_0-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\667-13th-21th_1220-24th-32th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\667-13th-21th_1220-24th-32th_1-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\667-13th-21th_1220-24th-32th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "667-13th-21th_1220-24th-32th [[0.10841148779763898, -0.17576214292785153], [0.19013283765525557, -0.10716291084372642]]\n",
      "[0.14927216272644728, -0.14146252688578898]\n",
      "[-0.246003125, -0.098843425] [-0.08212499999999999, -0.08437499999999999]\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\n",
      "Instructions for updating:\n",
      "\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "INFO:tensorflow:Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "I1105 22:31:30.137688  2324 music_vae_generate.py:145] Attempting to extract examples from input MIDIs using config `cat-mel_2bar_small`...\n",
      "\n",
      "2 valid inputs extracted from `data\\3-2. Output from MusicVAE\\371_trim.mid`. Outputting these potential inputs as `data\\3-2. Output from MusicVAE\\cat-mel_2bar_small_input1-extractions_2020-11-05_223130-*-of-002.mid`. Call script again with one of these instead.\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\371-39th-47th_1441-14th-22th_0-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\371-39th-47th_1441-14th-22th_0-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~10 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "Playing data\\3-2. Output from MusicVAE\\371-39th-47th_1441-14th-22th_1-of-2.mid\n",
      "\n",
      "MIDI file: data\\3-2. Output from MusicVAE\\371-39th-47th_1441-14th-22th_1-of-2.mid\n",
      "\n",
      "Format: 1  Tracks: 2  Divisions: 220\n",
      "\n",
      "Playing time: ~8 seconds\n",
      "\n",
      "Notes cut: 0\n",
      "\n",
      "Notes lost totally: 0\n",
      "\n",
      "\n",
      "371-39th-47th_1441-14th-22th [[0.03269705595448613, -0.20044396782759577], [0.11948666552416398, -0.28319781040772796]]\n",
      "[0.07609186073932506, -0.24182088911766186]\n",
      "[0.24982976249999997, 0.08807516249999998] [0.233375, -0.011375000000000001]\n"
     ]
    }
   ],
   "source": [
    "# test several times to find how labels changing\n",
    "testtimes = 10000\n",
    "for i in range(testtimes):\n",
    "    [music_1,music_2] = random.sample(train_list,2)\n",
    "    frag_1 = random.randint(0,52)\n",
    "    frag_2 = random.randint(0,52)\n",
    "    inter_result = interpolate_music(train_dir,output_dir,music_1,music_2,frag_1,frag_2)\n",
    "\n",
    "    eva_result = evaluate_wav_output(model,output_dir,inter_result)\n",
    "\n",
    "    print(inter_result,eva_result)\n",
    "    write_result(r\"data\\3-3. Emotion Changing result\\label_changing.csv\",inter_result,eva_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
